import argparse
import json
import sys
import os

from pathlib import Path

from engine.registry import get_agent
from engine.workflow import WorkflowEngine
from llm.chatgpt import ChatGPTLLM

def load_llm(model: str, model_type: str):
    if model == "chatgpt":
        return ChatGPTLLM(model=model_type)
    else:
        raise NotImplementedError(f"LLM model {model} is not supported yet.")
    
def load_input(args) -> dict:
    if args.input:
        return json.loads(args.input)
    elif args.file:
        code = Path(args.file).read_text(encoding="utf-8")
        return {"code": code}
    elif not sys.stdin.isatty():
        # í‘œì¤€ì…ë ¥ ì²˜ë¦¬
        code = sys.stdin.read()
        return {"code": code}
    else:
        raise ValueError("ì…ë ¥ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. --input, --file, stdin ì¤‘ í•˜ë‚˜ëŠ” í•„ìš”í•©ë‹ˆë‹¤.")

# langgrpah - workflow ì‹¤í–‰ì„ ìœ„í•´
def run_workflow(args):
    state = load_input(args)
    llm = load_llm(model=args.llm, model_type=args.model)

    engine = WorkflowEngine()

    def review_judge(state):
        if "bug" in state.get("code_review", ""):
            return "needs_fix"
        return "clean"

    engine.register_condition("review_judge", review_judge)
    engine.load_from_yaml(args.workflow, llm=llm)
    result = engine.run(state)

    if args.output:
        Path(args.output).write_text(json.dumps(result, indent=2, ensure_ascii=False), encoding="utf-8")
        print(f"âœ… ê²°ê³¼ ì €ì¥ë¨: {args.output}")
    else:
        print("ğŸ“‹ ê²°ê³¼:")
        for k, v in result.items():
            print(f"\n[{k}]\n{v}")
            
def run_single_agent(args):
    state = load_input(args)
    llm = load_llm(backend=args.llm, model=args.model)

    AgentClass = get_agent(args.agent)
    agent = AgentClass(llm=llm)

    result = agent(state)

    if args.output:
        Path(args.output).write_text(json.dumps(result, indent=2, ensure_ascii=False), encoding="utf-8")
        print(f"âœ… ê²°ê³¼ ì €ì¥ë¨: {args.output}")
    else:
        print("ğŸ“‹ ê²°ê³¼:")
        for k, v in result.items():
            print(f"\n[{k}]\n{v}")
            
def main():
    parser = argparse.ArgumentParser(description="General QA Agent CLI")
    subparsers = parser.add_subparsers(dest="command")

    # run workflow
    run_parser = subparsers.add_parser("run", help="ì›Œí¬í”Œë¡œìš° ì‹¤í–‰")
    run_parser.add_argument("--workflow", required=True, help="ì›Œí¬í”Œë¡œìš° YAML ê²½ë¡œ")
    input_group = run_parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--input", help="JSON ë¬¸ìì—´ ì…ë ¥")
    input_group.add_argument("--file", help="ì½”ë“œ íŒŒì¼ ê²½ë¡œ")
    run_parser.add_argument("--llm", default="chatgpt", help="LLM ë°±ì—”ë“œ (ê¸°ë³¸: chatgpt)")
    run_parser.add_argument("--model", default="gpt-3.5-turbo", help="LLM ëª¨ë¸")
    run_parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (json)")

    # run single agent
    agent_parser = subparsers.add_parser("run-agent", help="ë‹¨ì¼ Agent ì‹¤í–‰")
    agent_parser.add_argument("--agent", required=True, help="Agent ì´ë¦„ (registry ë“±ë¡ëœ ì´ë¦„)")
    agent_parser.add_argument("--llm", default="chatgpt", help="LLM ë°±ì—”ë“œ")
    agent_parser.add_argument("--model", default="gpt-3.5-turbo", help="LLM ëª¨ë¸")
    agent_parser.add_argument("--input", help="JSON ë¬¸ìì—´ ì…ë ¥")
    agent_parser.add_argument("--file", help="ì½”ë“œ íŒŒì¼ ê²½ë¡œ")
    agent_parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (json)")

    args = parser.parse_args()

    if args.command == "run":
        run_workflow(args)
    elif args.command == "run-agent":
        run_single_agent(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
